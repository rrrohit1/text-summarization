{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "from metrics import calculate_metrics\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/rohitrawat/job-prep/Assignments/accrete-ai/text-summarization/data/processed/news_summary_cleaned_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/accrete-ai/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "Your min_length=30 must be inferior than your max_length=28.\n",
      "/opt/miniconda3/envs/accrete-ai/lib/python3.11/site-packages/transformers/generation/utils.py:1282: UserWarning: Unfeasible length constraints: `min_length` (30) is larger than the maximum possible length (28). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n",
      "Your min_length=30 must be inferior than your max_length=15.\n",
      "/opt/miniconda3/envs/accrete-ai/lib/python3.11/site-packages/transformers/generation/utils.py:1282: UserWarning: Unfeasible length constraints: `min_length` (30) is larger than the maximum possible length (15). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df['generated_text_flan'] = df['text'].apply(lambda x: summarizer(x, max_length=min(300, len(x)//7), min_length=30, do_sample=False)[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"rrrohit/distilbart-cnn-12-6_finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['generated_text_bart'] = df['text'].apply(lambda x: summarizer(x, max_length=min(300, len(x)//7), min_length=15, do_sample=False)[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>generated_text_flan</th>\n",
       "      <th>generated_text_bart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>embarrassing loss vote face board cricket cont...</td>\n",
       "      <td>vinod rai head supreme courtappointed bccis co...</td>\n",
       "      <td>rai speaking launch biographical book cricket ...</td>\n",
       "      <td>bcci committee administrators cao vinod rai ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>new delhi mar pti missing defence personnel in...</td>\n",
       "      <td>external affairs minister sushma swaraj inform...</td>\n",
       "      <td>new delhi mar pti missing defence personnel in...</td>\n",
       "      <td>external affairs minister sushma swaraj said ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>saharanpur apr pti yoga guru ramdev said today...</td>\n",
       "      <td>yoga guru baba ramdev said patanjali ayurved f...</td>\n",
       "      <td>saharanpur yoga guru ramdev said patanjali ayu...</td>\n",
       "      <td>yoga guru ramdev friday said patanjali ayurved...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>supreme court today said aadhaar card cannot m...</td>\n",
       "      <td>supreme court said government cannot stopped u...</td>\n",
       "      <td>supreme court today said set sevenjudge bench ...</td>\n",
       "      <td>supreme court friday said aadhaar card cannot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>traffic tourist destinations shimla manali dal...</td>\n",
       "      <td>nearly tourists stranded kothi due road blocka...</td>\n",
       "      <td>narkanda jubbal kotkhai kharapathar chopal dis...</td>\n",
       "      <td>traffic tourist destinations shimla manali dal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1457  embarrassing loss vote face board cricket cont...   \n",
       "2862  new delhi mar pti missing defence personnel in...   \n",
       "2068  saharanpur apr pti yoga guru ramdev said today...   \n",
       "2078  supreme court today said aadhaar card cannot m...   \n",
       "2168  traffic tourist destinations shimla manali dal...   \n",
       "\n",
       "                                                summary  \\\n",
       "1457  vinod rai head supreme courtappointed bccis co...   \n",
       "2862  external affairs minister sushma swaraj inform...   \n",
       "2068  yoga guru baba ramdev said patanjali ayurved f...   \n",
       "2078  supreme court said government cannot stopped u...   \n",
       "2168  nearly tourists stranded kothi due road blocka...   \n",
       "\n",
       "                                    generated_text_flan  \\\n",
       "1457  rai speaking launch biographical book cricket ...   \n",
       "2862  new delhi mar pti missing defence personnel in...   \n",
       "2068  saharanpur yoga guru ramdev said patanjali ayu...   \n",
       "2078  supreme court today said set sevenjudge bench ...   \n",
       "2168  narkanda jubbal kotkhai kharapathar chopal dis...   \n",
       "\n",
       "                                    generated_text_bart  \n",
       "1457  bcci committee administrators cao vinod rai ur...  \n",
       "2862   external affairs minister sushma swaraj said ...  \n",
       "2068  yoga guru ramdev friday said patanjali ayurved...  \n",
       "2078  supreme court friday said aadhaar card cannot ...  \n",
       "2168  traffic tourist destinations shimla manali dal...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Flan T5-small model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/accrete-ai/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>BERTScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.326</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.253</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.263</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rouge-1  rouge-2  rouge-l  BERTScore\n",
       "r    0.326    0.162    0.289      0.630\n",
       "p    0.253    0.116    0.217      0.608\n",
       "f    0.263    0.123    0.230      0.616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(df.summary.tolist(), df.generated_text_flan.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuned DistilBART model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/accrete-ai/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>BERTScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.529</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.535</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.524</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rouge-1  rouge-2  rouge-l  BERTScore\n",
       "r    0.529    0.325    0.460      0.739\n",
       "p    0.535    0.330    0.462      0.745\n",
       "f    0.524    0.321    0.453      0.741"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(df.summary.tolist(), df.generated_text_bart.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accrete-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
